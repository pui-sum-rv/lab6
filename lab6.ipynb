{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferit-osirv/lab6/blob/main/lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "ipXpivMK_PT8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8vtuqXEBP1j"
      },
      "source": [
        "# Lab 6 -  Feature and Object Detection\n",
        "\n",
        "Ove laboratorijske vježbe se rješavaju u Google Colabu i spremaju na GitHub repozitorij koji je povezan na GitHub Classroom.\n",
        "\n",
        "## Kako riješiti zadatke?\n",
        "\n",
        "1. Prihvatite zadatak putem Google Classroom linka koji ćete dobiti. Google Classroom će kreirati repozitorij na vašem računu.\n",
        "2. Uđite u novokreiran repozitorij na vašem računu i kliknite na **.ipynb** datoteku, zatim kliknite **Open in Colab**.\n",
        "3. Zadatke rješavate u Google Colabu.\n",
        "\n",
        "## Kako spremiti (predati) zadatke?\n",
        "\n",
        "1. Unutar **Google Colaba** kliknite na **Open settings** kotačić u gornjem desnom kutu.\n",
        "2. Kliknite na **GitHub** tab i odaberite kvačicu za **Access private repositories and organizations**.\n",
        "3. Otvorit će se novi prozor da dodate pristup GitHubu. Kod **ferit-osirv** kliknite **Grant**.  \n",
        "4. Spremite i izađite iz postavki.\n",
        "\n",
        "\n",
        "5. Kliknite na **File > Save a copy in GitHub**.\n",
        "6. Odaberite kreiran repozitorij labosa **koji uključuje vaše ime**.\n",
        "\n",
        "> *Napomena:* Korake 1-4 morate napraviti samo prvi put."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feUPz7IDCbDx"
      },
      "source": [
        "## Kopiranje datoteka iz GitHub repozitorija\n",
        "\n",
        "Za izradu vježbi bit će vam potrebne slike i druge datoteke koje će se nalaziti u GitHub repozitoriju vježbe. Ovakva komanda će biti dostupna u notebooku svake vježbe. Ona će kopirati datoteke s GitHuba u Google Colab okruženje.\n",
        "\n",
        "**Ovu komandu je potrebno pokrenuti prije nego što krenete raditi svaku vježbu.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QpP_i0KgCefb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab38ff89-73f1-4a85-fcfb-25a23d5c4d91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'clone'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 25 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf clone && git clone https://github.com/ferit-osirv/lab6 clone && cp -a clone/. .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIPg8Vf9Cr8D"
      },
      "source": [
        "**Google Colab će povremeno obrisati sve datoteke.** Tako da će možda biti potrebno ponovno pokrenuti ovu komandu između dvije sesije. Ako dobivate greške da datoteke ne postoje, probajte ponovno pokrenuti gornju komandu."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction \n",
        "\n",
        "In this lab we will cover two topics: advanced feature detection extraction algorithms and object detection (using traditional image processing methods). \n",
        "\n",
        "Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. A characteristic of these large data sets is a large number of variables that require a lot of computing resources to process. Feature extraction is the name for methods that select and /or combine variables into features, effectively reducing the amount of data that must be processed, while still accurately and completely describing the original data set.\n",
        "\n",
        "Object detection is a technique that works to identify and locate objects within an image or video. In this way it provides better understanding and analisys of scenes in images and videos. Specifically, object detection draws bounding boxes around these detected objects, which allow us to locate where said objects are in (or how they move through) a given scene. With this kind of identification and localization, object detection can be used to count objects in a scene and determine and track their precise locations, all while accurately labeling them. \n",
        "\n"
      ],
      "metadata": {
        "id": "teTCkV7Blesu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Detection and Extraction\n",
        "\n",
        "What is feature?\n",
        "A local image feature is a tiny patch in the image that's invariant to image scaling, rotation and change in illumination.\n",
        "It's like the tip of a tower, or the corner of a window in the image above. Unlike a random point on the background (sky) in the image above,\n",
        "the tip of the tower can be precise detected in most images of the same scene. It is geometricly (translation, rotation, ...) and photometricly (brightness, exposure, ...) invariant.\n",
        "A good local feature is like the piece you start with when solving a jigsaw puzzle, except on a much smaller scale.\n",
        "It's the eye of the cat or the corner of the table, not a piece on a blank wall.\n",
        "The extracted local features must be:\n",
        "\n",
        "* Repeatable and precise so they can be extracted from different images showing the same object.\n",
        "* Distinctive to the image, so images with different structure will not have them.\n",
        "\n",
        "Due to these requirements, most local feature detectors extract corners and blobs. There is a wealth of algorithms satisfying the above requirements for feature detection (finding interest points on an image) and description\n",
        "(generating a vector representation for them). They include already learned Harris Corner Detection (in lab 4), and some more advanced algorithms, such as: \n",
        "\n",
        "* Scale Invariant Feature Transform (SIFT)\n",
        "* Speeded-Up Robust Features (SURF)\n",
        "* Features from Accelerated Segment Test (FAST)\n",
        "* Binary Robust Independent Elementary Features (BRIEF)\n",
        "* Oriented FAST and rotated BRIEF (ORB)\n",
        "\n",
        "The SIFT and SURF algorithms are patented by their respective creators, and while they are free to use in academic and research settings, you should technically be obtaining a license/permission from the creators if you are using them in a commercial (i.e. for-profit) application.\n",
        "\n",
        "Since is a known fact that ORB performs as well as SIFT on the task of feature detection (while outperforms SURF), in this lab, our focus will be on Oriented FAST and rotated BRIED (ORB)."
      ],
      "metadata": {
        "id": "w20Wr8quT4Pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oriented FAST and rotated BRIEF (ORB) \n",
        "\n",
        "Oriented FAST and rotated BRIEF (ORB) is a fast robust local feature detector. It is basically a fusion of FAST keypoint detector and BRIEF descriptor\n",
        "with many modifications to enhance the performance.\n",
        "\n",
        "ORB is a fusion of FAST keypoint detector and BRIEF descriptor with some added features to improve the performance. FAST is Features from Accelerated Segment Test used to detect features from the provided image. It also uses a pyramid to produce multiscale-features. Now it doesn’t compute the orientation and descriptors for the features, so this is where BRIEF comes in the role.\n",
        "\n",
        "ORB uses BRIEF descriptors but as the BRIEF performs poorly with rotation. So what ORB does is to rotate the BRIEF according to the orientation of keypoints. Using the orientation of the patch, its rotation matrix is found and rotates the BRIEF to get the rotated version. ORB is an efficient alternative to SIFT or SURF algorithms used for feature extraction, in computation cost, matching performance, and mainly the patents. SIFT and SURF are patented and you are supposed to pay them for its use. But ORB is not patented.\n",
        "\n",
        "\n",
        "We’ll start by showing the following figure that shows an example of using ORB to match between real world images with viewpoint change. Green lines are valid matches, red circles indicate unmatched points.\n",
        "\n",
        "ORB  uses an orientation compensation mechanism, making it rotation invariant while learning the optimal sampling pairs.\n",
        "\n",
        "### Orientation Compensation\n",
        "\n",
        "ORB uses a simple measure of corner orientation – the intensity centroid [5]. First, the moments of a patch are defined as:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://gilscvblog.files.wordpress.com/2013/10/figure2.jpg\">\n",
        "</p>\n",
        "\n",
        "With these moments we can find the centroid, the “center of mass” of the patch as:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://gilscvblog.files.wordpress.com/2013/10/figure3.jpg?w=300&h=116\">\n",
        "</p>\n",
        "\n",
        "We can construct a vector from the corner’s center O, to the centroid -OC. The orientation of the patch is then given by:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://gilscvblog.files.wordpress.com/2013/10/figure4.jpg?w=300&h=53\">\n",
        "</p>\n",
        "\n",
        "Here is an illustration to help explain the method:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://gilscvblog.files.wordpress.com/2013/10/angle.jpg\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Once we’ve calculated the orientation of the patch, we can rotate it to a canonical rotation and then compute the descriptor, thus obtaining some rotation invariance.\n",
        "\n",
        "We will now see ORB in action through examples and assigments! "
      ],
      "metadata": {
        "id": "N0y2w1bTFmOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try out the following code in order to find corners on the image"
      ],
      "metadata": {
        "id": "gQuNa77CFRdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 8] # enlarge inline plots \n",
        "\n",
        "# load input image\n",
        "img = cv2.imread('images/building_1.jpg')\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n",
        "plt.plot,plt.imshow(img)\n",
        "plt.title('Input Image'), plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o_3vCstfFMFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create ORB \n",
        "orb = cv2.ORB_create()\n",
        "# we can create ORB with specific number of key points we desire, for example, try to set them to 1000\n",
        "#orb = cv2.ORB_create(1000, 1.2)\n",
        "key_points, description = orb.detectAndCompute(img, None)\n",
        "print(\"Number of keypoints Detected: \", len(key_points))"
      ],
      "metadata": {
        "id": "XqvAFXYjMa4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draw rich key points on input image \n",
        "img_keypoints =cv2.drawKeypoints(img,key_points,img,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)"
      ],
      "metadata": {
        "id": "SNOMrVWpMxfm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draw result image:\n",
        "plt.plot,plt.imshow(img_keypoints)\n",
        "plt.title('ORB Interest Point'), plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Aki15NX-Mksw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The found interest points/features are circled in the image above.\n",
        "As we can see, some of these points are unique to this scene/building like the points near the top of the two towers. However, others like the ones at the top of the tree may not be distinctive. \n",
        "\n",
        "In the next assigment, you will try to extract the same features from a different image of the same cathedral taken from a different angle."
      ],
      "metadata": {
        "id": "-ntQlVFHNz2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assigment 1 - Feature Detection with ORB detector"
      ],
      "metadata": {
        "id": "3xGi5iosUzP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [20, 20] # enlarge inline plots "
      ],
      "metadata": {
        "id": "Sj8NQll8NzPO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and plot input image named 'building_1.jpg'\n",
        "#TODO:"
      ],
      "metadata": {
        "id": "drwYabOBOxil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load input image named 'building_2.jpg'\n",
        "#TODO:"
      ],
      "metadata": {
        "id": "Qwcm36_-VUea"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create ORB detector\n",
        "#TODO:\n",
        "orb = \n",
        "# find keypoints and their descriptors \n",
        "#TODO:\n",
        "key_points, description = \n",
        "# draw rich key points on input image \n",
        "#TODO:\n",
        "img_keypoints ="
      ],
      "metadata": {
        "id": "RBp9m_UDO1hW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature extraction function\n",
        "def image_detect_and_compute(detector, img_name):\n",
        "    # detect and compute intetrest points and their descriptors\n",
        "    img = cv2.imread(img_name)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    kp, des = detector.detectAndCompute(img, None)\n",
        "    return img, kp, des"
      ],
      "metadata": {
        "id": "cdtaicTIO5ol"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_image_matches(detector, img1_name, img2_name, nmatches=20):\n",
        "    # draw ORB feature matches of the given two images, using previously defined function\n",
        "    # TODO:\n",
        "    img1, kp1, des1 = \n",
        "    img2, kp2, des2 = \n",
        "\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    matches = bf.match(des1, des2)\n",
        "    matches = sorted(matches, key = lambda x: x.distance)\n",
        "    img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:nmatches],img2, flags=2)\n",
        "    # draw result images\n",
        "    plt.subplot(121),plt.imshow(img_keypoints)\n",
        "    plt.title('ORB Interest Point'), plt.xticks([]), plt.yticks([])\n",
        "    plt.subplot(122),plt.imshow(img_matches)\n",
        "    plt.title('Detector'), plt.xticks([]), plt.yticks([])\n",
        "    plt.show()\n",
        "# draw image matches\n",
        "orb = cv2.ORB_create()\n",
        "draw_image_matches(orb, 'images/building_1.jpg','images/building_2.jpg')\n"
      ],
      "metadata": {
        "id": "fN0x8a8gO7lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Object detection algorithms (P1)\n",
        "\n",
        "Broadly speaking, object detection can be broken down into 'traditional' image processing methods, and deep learning-based approaches.\n",
        "\n",
        "In more traditional based approaches, computer vision techniques are used to look at various features of an image, such as the color histogram or edges, to identify groups of pixels that may belong to an object. \n",
        "\n",
        "On the other hand, deep learning-based approaches employ convolutional neural networks (CNNs) to perform end-to-end, unsupervised object detection, in which features don’t need to be defined and extracted separately. \n",
        "\n",
        "For now, in this lab we will focus on traditional image detection methods, while deep learning methods which have become the state-of-the-art approaches for object detection, will be covered in further laboratory excersises. "
      ],
      "metadata": {
        "id": "Nk-M0j7ToRmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Template Matching Algorithm\n",
        "\n",
        "Template Matching is a method for searching and finding the location of a template image in a larger image. \n",
        "\n",
        "In OpenCV, template matching is implemented inside ```cv2.matchTemplate``` function. \n",
        "\n",
        "So what exactly is the ```cv2.matchTemplate``` function doing? Essentially, this function takes a “sliding window” of our query image and slides it across our puzzle image from left to right and top to bottom, one pixel at a time. Then, for each of these locations, we compute the correlation coefficient to determine how “good” or “bad” the match is.\n",
        "\n",
        "Regions with sufficiently high correlation can be considered “matches” for our image template. There are a variety of methods to perform template matching, \n",
        "based on different correlation coefficients. \n",
        "\n",
        "Definitions of different correlation coefficients can be found at: \n",
        "https://docs.opencv.org/2.4/modules/imgproc/doc/object_detection.html\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ZMH3pFrQ_vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assigment 2 - Object detection using template matching"
      ],
      "metadata": {
        "id": "fiXZwj7Clagx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 8] # enlarge inline plots\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "VFae9b_wSh1S"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load full input image \n",
        "full = cv2.imread('images/cat.jpg')\n",
        "full = cv2.cvtColor(full, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(full)"
      ],
      "metadata": {
        "id": "jnwQmLS0dLnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Template image\n",
        "# This is a subset of the previous image, note how its actually the exact image\n",
        "template = cv2.imread('images/cat_face.jpg')\n",
        "template = cv2.cvtColor(template, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(template)"
      ],
      "metadata": {
        "id": "RztesRCkjbYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "height, width,channels = template.shape"
      ],
      "metadata": {
        "id": "TIyNa72KTrzS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 6 different template matching methods\n",
        "# TODO:\n",
        "methods = []"
      ],
      "metadata": {
        "id": "bC3k2d_qjc0q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in methods:\n",
        "    \n",
        "    # Create a copy of the image\n",
        "    full_copy = full.copy()\n",
        "    \n",
        "    # Get the actual function instead of the string\n",
        "    method = eval(m)\n",
        "\n",
        "    # apply template matching algorithm\n",
        "    res = cv2.matchTemplate(full_copy,template,method)\n",
        "    \n",
        "    # grab the Max and Min values, plus their locations\n",
        "    #TODO:\n",
        "    min_val, max_val, min_loc, max_loc = \n",
        "        \n",
        "    # if the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n",
        "    \n",
        "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
        "    #TODO: \n",
        "    else:\n",
        "    #TODO: \n",
        "        \n",
        "    # assign the bottom right of the rectangle\n",
        "    bottom_right = (top_left[0] + width, top_left[1] + height)\n",
        "    # draw the red rectangle\n",
        "    cv2.rectangle(full_copy,top_left, bottom_right, 255, 10)\n",
        "\n",
        "    # Plot the Images\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.imshow(res)\n",
        "    plt.title('Result of Template Matching')\n",
        "    \n",
        "    plt.subplot(1,2,2)\n",
        "    plt.imshow(full_copy)\n",
        "    plt.title('Detected Point')\n",
        "    plt.suptitle(m)  \n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LerYqIjxjeRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ne zaboravite spremiti zadatke na GitHub!"
      ],
      "metadata": {
        "id": "t0OmOXItEjYK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab6.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
