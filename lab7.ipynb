{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferit-osirv/lab7/blob/main/lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "ipXpivMK_PT8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8vtuqXEBP1j"
      },
      "source": [
        "# Lab 7 -  Intro to Convolutional Neural Networks\n",
        "\n",
        "Ove laboratorijske vježbe se rješavaju u Google Colabu i spremaju na GitHub repozitorij koji je povezan na GitHub Classroom.\n",
        "\n",
        "## Kako riješiti zadatke?\n",
        "\n",
        "1. Prihvatite zadatak putem Google Classroom linka koji ćete dobiti. Google Classroom će kreirati repozitorij na vašem računu.\n",
        "2. Uđite u novokreiran repozitorij na vašem računu i kliknite na **.ipynb** datoteku, zatim kliknite **Open in Colab**.\n",
        "3. Zadatke rješavate u Google Colabu.\n",
        "\n",
        "## Kako spremiti (predati) zadatke?\n",
        "\n",
        "1. Unutar **Google Colaba** kliknite na **Open settings** kotačić u gornjem desnom kutu.\n",
        "2. Kliknite na **GitHub** tab i odaberite kvačicu za **Access private repositories and organizations**.\n",
        "3. Otvorit će se novi prozor da dodate pristup GitHubu. Kod **ferit-osirv** kliknite **Grant**.  \n",
        "4. Spremite i izađite iz postavki.\n",
        "\n",
        "\n",
        "5. Kliknite na **File > Save a copy in GitHub**.\n",
        "6. Odaberite kreiran repozitorij labosa **koji uključuje vaše ime**.\n",
        "\n",
        "> *Napomena:* Korake 1-4 morate napraviti samo prvi put."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Convolutional neural networks (CNNs) are one of the largest revolutions in the computer vision field. They are the state of the art (SOTA) solution for problems like object detection, handwriting recognition, face recognition, and many other image processing tasks.\n",
        "\n",
        "Traditional methods are still used for specific tasks and as preprocessing steps for CNNs, so don't forget everything you've learned so far. :)\n",
        "\n",
        "You already used convolutions many times in these labs. In lab2 you wrote your own convolution code. In lab3 you used median filtering, a convolution with a specific kernel to blur the image. In lab4 you used a blur filter.\n",
        "\n",
        "Depending on the used kernel, convolutions can achieve all kinds of different things: Show specific features like edges and corners, show and remove textures, etc.\n",
        "\n",
        "Traditionally, when someone is working on an image processing task they would hand-select the kernel used in the convolution.\n",
        "\n",
        "CNNs work similarly but take it all one step further. Instead of manually convolving the image with known kernels, in deep learning we let the network learn the kernels it's using. CNNs have multiple layers, in each layer, they perform lots and lots of convolutions with different kernels in parallel. Each layer's output is used as input for the next layer. By doing this, a CNN can learn more-and-more complex features deeper in the network. At first, it can detect lines and corners, in the later layers, it can combine lines to detect more complex shapes, and so on.\n",
        "\n",
        "Follow along this notebook to find out how CNNs work."
      ],
      "metadata": {
        "id": "w20Wr8quT4Pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch and neural networks\n",
        "\n",
        "For this lab, you'll use PyTorch, one of the most popular deep learning libraries to implement a convolutional neural network from scratch. For simple problems, you can use tools like [fastai](https://docs.fast.ai) which can automate a lot of this process for you. Still, it's good to be familiar with what lies beneath tools like that.\n",
        "\n",
        "Begin by importing PyTorch and the TorchVision library, which has useful functions to work with images."
      ],
      "metadata": {
        "id": "QA5LbrXBpl1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "grSKQ8-xp4Og"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll declare a couple of properties. Don't worry about these just yet."
      ],
      "metadata": {
        "id": "WCwjPo2hp6dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 3\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 100"
      ],
      "metadata": {
        "id": "oq8wPJ2Bp9QX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing our data\n",
        "\n",
        "To train a neural network, we first need a lot of data. Neural networks learn by looking at example input images and their correct output. In your case that will be a bunch of handwritten digits and their correct translation into text.\n",
        "\n",
        "You can look at a neural network as a math function, mapping inputs into outputs. By knowing the correct solution during training, a neural network can adjust what it does inside the function so that its output matches the correct solution.\n",
        "\n",
        "For this problem, you'll use a popular dataset of handwritten digits called MNIST. Torchvision already includes a function to download the dataset.\n",
        "\n",
        "You'll apply a transformation to each image. First, you'll convert the images to PyTorch Tensors. Tensors are _very_ similar to NumPy arrays, so you're already fairly familiar with them.\n",
        "\n",
        "The other transformation you'll apply is normalization. Since neural networks work by multiplying and adding together a bunch of numbers, the absolute values of those numbers matter a lot. Multiplying 0.001 a bunch of times will give you a very small number at the end, making it very hard to work with. Similarly, multiplying huge numbers can be unstable too. It's best to map CNN inputs into a range of [-0.5, 0.5], making sure they're centered around 0. That's what the Normalize transform does.\n",
        "\n",
        "You'll create two datasets, one for training and one for testing. It's very important that you do this step: You can't verify that your model works if you train and test the model on the same data, since it can in some cases learn to reproduce correct outputs on the training data by memorization, not understanding."
      ],
      "metadata": {
        "id": "_Z84eHIsp_oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "  torchvision.transforms.ToTensor(),\n",
        "  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = torchvision.datasets.MNIST('/files/', train=True, \n",
        "                                        download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size_train, shuffle=True) \n",
        "\n",
        "test_data = torchvision.datasets.MNIST('/files/', train=False, \n",
        "                                        download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test, shuffle=True)"
      ],
      "metadata": {
        "id": "B5LTxvPPqAyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You also create a DataLoader for your dataset. NNs are usually trained in batches of inputs. Training the network one by one input is far too slow, but we also can't train on all of the inputs. That's why we specify a batch size of 64: During each training iteration, the network will receive 64 images. The DataLoader class does the batching for us.\n",
        "\n",
        "Let's grab the first batch from the testing DataLoader:"
      ],
      "metadata": {
        "id": "plMMTRx1qZ3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "example_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgxF_zLxqbNc",
        "outputId": "d08ab439-0167-430f-a592-f415a2621829"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your example data has the following shape: (n_batches, n_channels, width, height). The batch size is specified when creating the DataLoader. The images themselves are very small and grayscale, which explains why they only have one channel.\n",
        "\n",
        "**Question 1:** What is the shape of the _training_ loader's first batch?\n",
        "\n",
        "Type your answer here."
      ],
      "metadata": {
        "id": "b7_vTM5hqc6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's use matplotlib to plot a few examples of the images:"
      ],
      "metadata": {
        "id": "Lt7uvmghqiLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(example_targets[i].item())\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "metadata": {
        "id": "T2JiCSzNqjdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**: You were tasked with training a NN for handwritten digit detection using this dataset. You will perform the detection on scanned (regular A4 printer) papers where the digits were typed inside boxes with a blue pen. Which preprocessing steps would you perform on your own inputs, knowing the network was trained on images like the ones above?\n",
        "\n",
        "Type your answer here."
      ],
      "metadata": {
        "id": "PAkIntmWqqvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN architecture\n",
        "\n",
        "Now it's time to build the network. A CNN usually consists of convolutional layers, i.e. NN layers that perform a convolution on the input matrix and return the convolved matrix as output. You'll use two of these layers. You'll then use a dropout layer, which just randomly disables half of the features to prevent the network from memorizing the training dataset. Finally, you'll use two layers to combine all of the convolution results into just 10 numbers, the probability for each digit. Exactly how those layers are combined (whether they are summed, how much each layer contributes to the result for each digit, etc.) will be learned during training."
      ],
      "metadata": {
        "id": "zXVuB_0Uquea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "UMgr65FEqwza"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the `__init__` of the class you create all the necessary layer objects that the network will use, but you just add them as properties, they don't do anything yet.\n",
        "\n",
        "The `forward()` method is called every time the network receives a new output. `forward()`'s job is to calculate the output from the network and return it. What you'll do is pass the input to the first layer, and then pass the output of that layer to the next one, all the way until you get your 10 numbers. Finally, you'll use `log_softmax()` on those 10 numbers just to map them to probabilities from 0 to 1, and make sure they all add up to 1. There's no magic here, just high-school level math. :)\n",
        "\n",
        "Next, we'll instantiate the network on the GPU:"
      ],
      "metadata": {
        "id": "fP10ysppqy98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "network = Net().to(device)\n",
        "\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                      momentum=momentum)"
      ],
      "metadata": {
        "id": "IVkiW_FWq19l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Along with the network, we also instantiate an optimizer. The optimizer is in charge of training the network, more on that in the next section:"
      ],
      "metadata": {
        "id": "AG5usvyUq5Qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Now that you have a network, it's time to train it. Explained at a very high level, training works by first giving the model an input image, and the model gives us a resulting prediction in the form of 10 numbers, a probability for each digit. Naturally, this prediction is basically random at first. We then compare the model's output with the actual value (for the digit 3, it would be `[0, 0, 0, 1, 0, 0, ...]` - the probability of 1 on the 4th index of the array). The comparison is done with a function called the **loss** function, which measures how bad the model is. During training, our job is to **minimize the loss function**.\n",
        "\n",
        "To minimize the loss, we simply need to shift the parameters of the network so that the loss goes down. We can know this if we calculate the gradient of the loss. You already know from math that the gradient measures the speed and direction of change. By taking the gradient of the loss function, we know if we need to increase or decrease our parameters to get a decrease of the loss.\n",
        "\n",
        "That's where the optimizer comes in. PyTorch includes a bunch of different algorithms to minimize the loss function. They are in charge of increasing or decreasing the network's parameters so that the loss goes down. That's why the optimizer above receives the network's parameters: It needs to be able to change them.\n",
        "\n",
        "The following function will be called during each **epoch** of training. An epoch is _one iteration through the whole training dataset_. In real-world applications, we use 100s of epochs, but in this example, only 3 are enough to achieve good results."
      ],
      "metadata": {
        "id": "A8z2qMmkq66R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  # Set the network in training mode. This doesn't really do much, it's just\n",
        "  # a flag that we set.\n",
        "  network.train()\n",
        "  # for each training batch in the train loader\n",
        "  for batch_idx, data in enumerate(train_loader):\n",
        "    # grab the training batch and copy it to the GPU\n",
        "    # image - the input image\n",
        "    # target - the correct output\n",
        "    image, target = data[0].to(device), data[1].to(device)\n",
        "    # first, set all gradients to zero\n",
        "    optimizer.zero_grad()\n",
        "    # calculate a prediction from the model\n",
        "    output = network(image)\n",
        "    # calculate the loss for this batch\n",
        "    loss = F.nll_loss(output, target)\n",
        "    # calculate the gradient of each part of the network,\n",
        "    # and store it inside the tensors in the network\n",
        "    loss.backward()\n",
        "    # then use those gradients to nudge paramers so that the loss goes down\n",
        "    optimizer.step()\n",
        "\n",
        "    # this part doesn't perform training, it just gives us a progress report\n",
        "    # and periodically saves our training result\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      torch.save(network.state_dict(), 'model.pth')\n",
        "      torch.save(optimizer.state_dict(), 'optimizer.pth')"
      ],
      "metadata": {
        "id": "xMpa4uJxq8hd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we're done with training, we're ready to test our network. We'll do this by giving the model our test data and taking all the predictions. We then compare the model's predictions with known correct outputs to calculate the model's accuracy."
      ],
      "metadata": {
        "id": "hJGSbx5Oq-5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "      image, target = data[0].to(device), data[1].to(device)\n",
        "      output = network(image)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      # our output gives us a probability for each digit, so we take the one\n",
        "      # with the highest probability as our predicted digit\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "38U7xU6grAh3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far you only declared functions, it's time to use them. You'll set up a couple of variables to track the loss so that you can plot it later, and then train for three epochs, and test the model at the end of each epoch."
      ],
      "metadata": {
        "id": "FRq9XkiZrCH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "\n",
        "test()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "id": "t7GJ2dgnrDSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have a trained model!"
      ],
      "metadata": {
        "id": "qNrXY9dPrgO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Model's Performance\n",
        "\n",
        "Let's plot the training curve:"
      ],
      "metadata": {
        "id": "jelXPFceridA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(train_counter, train_losses, color='blue')\n",
        "plt.scatter(test_counter, test_losses, color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('number of training examples seen')\n",
        "plt.ylabel('negative log likelihood loss')"
      ],
      "metadata": {
        "id": "jAXDfUbdrjut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get most of the training done right at the start, but going from 20% to 80% accuracy is easy. The closer we get to 100%, the harder it is to get additional accuracy.\n",
        "\n",
        "\n",
        "Let's take a look at some examples:"
      ],
      "metadata": {
        "id": "iGcXH0whrl-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  output = network(example_data.to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO8HtrUnrnVp",
        "outputId": "20050713-0f16-4fea-b7a7-142ff5f6996b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Prediction: {}\".format(\n",
        "    output.data.max(1, keepdim=True)[1][i].item()))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "metadata": {
        "id": "4aYTts8trpqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model guesses the numbers correctly! Let's see the accuracy for each number:"
      ],
      "metadata": {
        "id": "nVZXXB25ruGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [str(i) for i in range(11)]\n",
        "class_correct = list(0. for i in range(11))\n",
        "class_total = list(0. for i in range(11))\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = network(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s : %2d %%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "metadata": {
        "id": "7kTxK3dNrvjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** Which numbers have the worst accuracy? Can you guess why? Knowing the accuracy of the model is about 97%, would you say this is comparable to human-level accuracy for the same task?\n",
        "\n",
        "Type your answer here."
      ],
      "metadata": {
        "id": "rxoGyhKjrybD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ne zaboravite spremiti zadatke na GitHub!"
      ],
      "metadata": {
        "id": "t0OmOXItEjYK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of lab6.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
